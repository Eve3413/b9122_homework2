{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "416f7283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from urllib.request import Request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db89d91",
   "metadata": {},
   "source": [
    "### Question 1 Part 1\n",
    "In this question you are tasked with writing modified versions of the web crawler that we covered in class.\n",
    "More specifically, you asked to create two webcrawlers for the following tasks: \n",
    "\n",
    "1. Crawl pages whose seed url is the press releases page of the Federal Reserve System :https://www.federalreserve.gov/newsevents/pressreleases.htm and collect pages that contain the\n",
    "word “covid” found within the page. The goal is to collect at least 10 such urls. At the end of the crawling the code should output the urls of the webpages found to contain the word “covid”. When checking whether the word is present on the webpage you should consider lower- and upper-case word versions (Covid, COVID, covid). One way to do this is to lowercase the webpage text prior to doing word matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "e0681bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_url = \"https://www.federalreserve.gov/newsevents/pressreleases.htm\"\n",
    "urls = []   \n",
    "covid_urls = []\n",
    "strToFind = \"covid\"\n",
    "maxNumUrl = 15\n",
    "cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "58c01dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = urllib.request.Request(curr_url,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urllib.request.urlopen(req).read()\n",
    "soup = BeautifulSoup(webpage)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "ba33906c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling End\n"
     ]
    }
   ],
   "source": [
    "for tag in soup.find_all('a', href = True): #find tags with links\n",
    "    childUrl = tag['href'] #extract just the link\n",
    "    childUrl = urllib.parse.urljoin(seed_url, childUrl)\n",
    "    urls.append(childUrl)\n",
    "       \n",
    "for url in urls:\n",
    "    if cnt > maxNumUrl:\n",
    "        print(\"Crawling End\")\n",
    "        break\n",
    "    try:\n",
    "        c_req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        c_soup = BeautifulSoup(urllib.request.urlopen(c_req).read(), \"html.parser\")\n",
    "        if (strToFind in c_soup.get_text().lower()) and (url not in covid_urls):\n",
    "            covid_urls.append(url)\n",
    "            cnt = cnt + 1\n",
    "    except:\n",
    "        continue        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "96612376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.federalreserve.gov/default.htm',\n",
       " 'https://www.federalreserve.gov/feeds/feeds.htm',\n",
       " 'https://www.federalreserve.gov/publications.htm',\n",
       " 'https://www.federalreserve.gov/sitemap.htm',\n",
       " 'https://www.federalreserve.gov/azindex.htm',\n",
       " 'https://www.federalreserve.gov/faqs.htm',\n",
       " 'https://www.federalreserve.gov/aboutthefed/procurement/about.htm',\n",
       " 'https://www.federalreserve.gov/monetarypolicy/policytools.htm',\n",
       " 'https://www.federalreserve.gov/monetarypolicy/review-of-monetary-policy-strategy-tools-and-communications.htm',\n",
       " 'https://www.federalreserve.gov/supervisionreg.htm',\n",
       " 'https://www.federalreserve.gov/supervisionreg/topics/topics.htm']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8911f7",
   "metadata": {},
   "source": [
    "### Question 1  Part 2\n",
    "2. Crawl pages whose seed url is the press releases page of the Securities and Exchange Commission: https://www.sec.gov/news/pressreleases and collect urls of press releases that contain the word “charges”. The code should output the first 20 such links that it finds. For each link output the url and the text. Similar to the previous task, when checking for the presence of the word “charges” you should consider lower- and upper-case versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "172177dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_url = \"https://www.sec.gov/news/pressreleases\"\n",
    "urls = []   \n",
    "charges_urls = []\n",
    "strToFind = \"charges\"\n",
    "maxNumUrl = 20\n",
    "cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ff8a10d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = urllib.request.Request(curr_url,headers={'User-Agent': 'Mozilla/5.0'}) \n",
    "webpage = urllib.request.urlopen(req).read() \n",
    "soup = BeautifulSoup(webpage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9ba5dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in soup.find_all('a', href = True): #find tags with links\n",
    "    childUrl = tag['href'] #extract just the link\n",
    "    childUrl = urllib.parse.urljoin(seed_url, childUrl)\n",
    "    urls.append(childUrl)\n",
    "       \n",
    "for url in urls:\n",
    "    if cnt > maxNumUrl:\n",
    "        print(\"Crawling End\")\n",
    "        break\n",
    "    try:\n",
    "        c_req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        c_soup = BeautifulSoup(urllib.request.urlopen(c_req).read(), \"html.parser\")\n",
    "        if (strToFind in c_soup.get_text().lower()) and (url not in covid_urls):\n",
    "            covid_urls.append(url)\n",
    "            cnt = cnt + 1\n",
    "    except:\n",
    "        continue     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a22bdcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
